---
title: "Introduction to TailorBayes"
bibliography: /Users/skarapanagiotis/Box/PhD/Code/Tailored_Bayes/TailorBayes/inst/references.bib
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{my-vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(TailorBayes)
```

\marginnote{This document refers to ggmcmc version `r packageVersion("TailorBayes")`.}

# Introduction 

The package provides functions for tailored Bayesian (TB) modelling, a simple and widely applicable approach to incorporate misclassification costs into Bayesian inference.

Routinely, binary classifiers aim to minimise the expected classification error; that is the proportion of incorrect classifications.

The disadvantage of this paradigm is to implicitly assume that all misclassification
errors have equal costs. However, equality is but one choice, which may be rarely appropriate. 

For example, in cancer diagnosis, a false negative (that is, misdiagnosing a cancer patient as healthy) could have more severe consequences than a false positive (that is, misdiagnosing a healthy patient with cancer); the latter may lead to extra medical costs and unnecessary patient anxiety but will not result in loss of life. 

Add another example, eg bank 

For these applications, a prioritised control of asymmetric classification errors is
more appropriate.

Tailor Bayes (TB), allows for the incorporation of 
misclassifcation cocts into Bayesian modelling. In a nutshell, this is done by 
intorducing datapoint-specific weights and 
we use these weights to downweighting each datapoint's likelihood contribution when fitting the model.


# The model

We utilise data $D = \{ (y_i, x_i)  :i=  1, \dots, n \}$ $(X, Y) \in \mathbb{R}^d \times \{0, 1\}$ where $y_i$ is the binary outcome variable indicating the class to which the $i^{th}$ observation belongs and $x_i$ is the vector of covariates of size $d$. The same notation is used interchangeably for scalar and vector-valued quantities. 

We use a generalised version of the logistic loss 

\begin{equation}
   \ell(y_i, x^T_i \beta) = -(\pi_w(x_i;\beta))^{y_i} (1- \pi_w(x_i;\beta))^{1-y_i},
   \label{hand_loss}
\end{equation}

where $\pi_w(x_i;\beta)=  (\exp \{ {x^T_i \beta} \}/ 1 + \exp\{ {x^T_i \beta}\})^{w_i}$ and $w_i \in[0,1]$ are datapoint-specific weights. We recover the standard logistic loss by setting $w_i = 1$ for all $i = 1, \dots ,n$. Note that we specify a linear function, ie $x^T_i \beta$, where $\beta$ is a $d + 1$ dimensional vector of regression coefficients. Hence, our objective is to learn $\beta$.

We set the datapoint-specific weights as 

\begin{equation}
    w_i = \exp \big \{-\lambda h(\pi_u(x_i), t) \big \} = \exp \big \{-\lambda (\pi_u(x_i) - t)^2 \big \},
    \label{weights}
\end{equation}

where $h$ is the quadratic loss (this is the default option, other options discussed later), $\pi_u(x_i)$ is the unweighted version of $\pi_w(x_i;\beta)$, and $t$ is the target threshold that summarises the the costs and benefits of (mis-)classifications. Using this formulation the weights decrease with increasing distance from the target threshold. The hyperparameter $\lambda \ge 0$ is a "discounting factor" that controls the rate of that decrease.  For $\lambda = 0$  we recover the standard logistic regression model. 

To costruct the weights we need to specify the following ingredients: 

- $t$,  

- $\lambda$. We recommend using cross-validtion. 

- $\pi_u(x_i)$. We achieve this by a two-stage procedure. First, the distance is measured using an estimate of $\pi_u(x_i)$, $\hat{\pi}_u(x_i)$, which
can be compared with $t$ to yield weights. This estimate could be based on any classification
method: we use standard unweighted Bayesian logistic regression in the analysis or any other classifiers that output probability estimates. 

To finish the formulation of the model we assume indepedent normal prior distribution for each element of $\beta$, ie. $p(\beta) = \mathcal{N}(0, \sigma^2I)$. The TB posterior is proportional to

$$p(\beta|D) \propto -\sum_{i=1}^{n} log(\ell(y_i, x^T_i \beta)) p(\beta).$$
Since this posterior is not analytically tractable, we use Markov Chain Monte Carlo (MCMC) for inference, using the `metrop_tailor()` function. 

# Data

```{r}
n <- 200
rho <- 0.5
beta0 <- 0.25
beta1 <- 1
beta2 <- 0.5

x1 <- rnorm(n)
x2 <- rho * x1 + sqrt(1 - rho ^ 2) * rnorm(n)
eta <- beta0 + beta1 * x1 + beta2 * x2
p <- 1 / (1 + exp(- eta))
y <- as.numeric(runif(n) < p)

data <- data.frame(y  = y, x1 = x1, x2 = x2)
```

```{r}
# split into design and train
#install.packages("splitstackshape")
library(splitstackshape)
data_split <- stratified(data, group = "y", size = 0.2, bothSets = T)

data_design <- data_split$SAMP1
data_train <- data_split$SAMP2
```

# `metrop_tailor()`
This is the main function 

random walk MCMMC

At a high level, it needs as input the data and the ingredients for the cosntruction of the weights,



First, we will use 20% of the data to estimate $\pi_u(x_i)$. I use frequentist logistic regression for this. 

```{r}
fit <- glm(y ~ x1 + x2, family = binomial("logit"))
pred <- predict(fit, newdata = data_train, type = "response")
```

Then, we can use CV to find a good $\lambda$ value. I skip this step here. The value pf $t$ needs to set as well. I use $t = 0.3$ here. 

```{r}
fit_tailor <- metrop_tailor(y ~ x1 + x2, data = data_train, lambda = 10, pi_u = pred, t = 0.3)
```

Now, `fit_tailor` is a list object, where the first element, `chain` is of class "mcmc". This element can be summarized by functions provided by the [coda](https://cran.r-project.org/web/packages/coda/index.html) package or similar ones, such as [ggmcmc](https://cran.r-project.org/web/packages/ggmcmc/ggmcmc.pdf) [@Xavier2016]. 

For instance, we can plot  daignostic sucha as traceplots.
```{r}
library(coda)
traceplot(fit_tailor$chain)
```

# Customisation 

The package allows for flexibility on the construction of the weights, $w_i$ and the specifiction of the prior density. 

## Custom weights
The construction weights is controlled by the `distance_measure` and `espilon` arguments. Together they allow the specification of a family of weigthing functions, termed $\epsilon$-insensitive functions \cite{vapnik1998statistical}. The family is defined as

$$ h(\pi_u(x), t, \epsilon) = (\pi_u(x) - t)^2_{\epsilon}$$

where we denote 
\begin{equation}
 (\pi_u(x) - t)^2_{\epsilon} = 
  \begin{cases} 
   0 		& \text{if } (\pi_u(x) - t)^2 \leq \epsilon \\
   (\pi_u(x) - t)	- \epsilon	& \text{otherwise }   
  \end{cases}
\label{epsilon_insensitive}
\end{equation}

This is the $\epsilon$-insensitive squared loss. 
The $\epsilon$-insensitivity  arises  from  the  fact  that  the  loss  is  equal  to  0  if  the  discrepancy  between  the predicted  probability $\pi_u(x)$  and  the  target  threshold $t$ is  less  than $\epsilon$.   In  other  words,  we  do  not  careabout the distance as long as it is less than $\epsilon$,  but will not accept any deviation larger than this. 

Another option is to choose the $\epsilon$-insensitive ablssoute loss (`distance_measure = "absolute"`), defined as 

$$ h(\pi_u(x), t, \epsilon) = |\pi_u(x) - t|_{\epsilon} $$

The default is the squared loss with $\epsilon = 0$, i.e
$h(\pi_u(x), t, \epsilon) = (\pi_u(x) - t)^2$. 

### Further customisation
In addtion, the user can define an arbitrary weigthing function, $h$. This is done with using the `h_function` argument. 

An example is given below. For illustration purposes, we simply re-define the square and absolute losses. 

In order to avoid using either global variables or $\dots$ arguments, we use the function factory pattern (Chapter 10, @wickham_advanced), and [Geyer, 2020 Appendix A](https://cran.r-project.org/web/packages/mcmc/vignettes/demo.pdf). 
This is a robust way to pass information to a function being
passed to another R function (a higher-order function), for example when
the function being passed is the weighting function for a
simulation done by the higher-order function, which simulates the distribution having that using that  weighting function (R function `metrop_tailor()` in this package for example). 
We define the custom weighting function as 

```{r}
my_h_function <- function(indicator) function(pi_u, t){
    if(indicator == 1){
        (pi_u - t) ^ 2
    } else {
        abs(pi_u - t)
    }
}
```

This allows more flexibilty since the user can specify any number of extra arguments in the fist function call. The second should always be as specified above. 
The user then simply uses the `h_function` argument of `metrop_tailor()`, specifying any additional arguments as well. Uisng my custom funciton, I want to implement the sqare loss, so I need to specify `indicator = 1` 

```{r}
fit_tailor_h_function <- metrop_tailor(y ~ x1 + x2, data = data_train, lambda = 10, pi_u = pred, t = 0.3, h_function = my_h_function(indicator = 1))
```

In case there no extra arguments, we can define the weigthing fucntion as 

```{r}
my_h_function_2 <- function() function(pi_u, t){
  (pi_u - t) ^ 2
  }
```

```{r}
fit_tailor_h_function_2 <- metrop_tailor(y ~ x1 + x2, data = data_train, lambda = 10, pi_u = pred, t = 0.3, h_function = my_h_function_2())
```

## Custom prior
As described above the default prior for $\beta$ is indepedent normal prior distribution, ie. $p(\beta) = \mathcal{N}(\mu, \sigma^2I)$, with default values $\mu = 0$ and $\sigma = 1000$. 

The user can set the mean and standard deviation, using the `prior_mean` and `prior_sd` arguments of `metrop_tailor()`. 

In addition, the user can specify any prior desnity using the `user_prior_density` argument. 
If non-NULL, the prior (log)density up to a constant of proportionality. This
must be a function defined in R.  

```{r}
 ## user-defined independent Cauchy prior
logpriorfun <- function(beta){
     sum(dcauchy(beta, log = TRUE))
}

fit_tailor_cauchy_1 <- metrop_tailor(y ~ x1 + x2, data = data_train, lambda = 10, pi_u = pred, t = 0.3, user_prior_density = logpriorfun)
```

It follows the same function factory idea as above. This means 

```{r}
## user-defined independent Cauchy prior with additional args
 logpriorfun <- function(location, scale) function(beta){
    sum(dcauchy(beta, location, scale, log = TRUE))
 }

fit_tailor_cauchy_2 <- metrop_tailor(y ~ x1 + x2, data = data_train, lambda = 10, pi_u = pred, t = 0.3,  user_prior_density = logpriorfun(location = 0, scale = 5))
```


```{r}
library(ggmcmc)
library(purrr)
combined <- list(fit_tailor_cauchy_1$chain, fit_tailor_cauchy_2$chain)
ggs_obj_convert <- map(combined, ggs)
names(ggs_obj_convert) <- c("Cauchy prior", "Cauchy prior additional args")
ggs_caterpillar(ggs_obj_convert)
```

# References
